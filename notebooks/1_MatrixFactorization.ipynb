{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_MatrixFactorization.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Factorization"
      ],
      "metadata": {
        "id": "R1GANC7gJw_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "U1AtJGEgLyLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlpEfYaILqX1"
      },
      "outputs": [],
      "source": [
        "!wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip ml-latest-small.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "4R8MQTMOPefJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = True\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "embedding_dim = 16\n",
        "epochs = 500\n",
        "device = \"cpu\" if not use_gpu else \"cuda\""
      ],
      "metadata": {
        "id": "Pw8MSuxOPf6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and initialization"
      ],
      "metadata": {
        "id": "PIBAq0-FMYM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "cNSch2__Mdnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df = pd.read_csv('ml-latest-small/ratings.csv')"
      ],
      "metadata": {
        "id": "laHy6kkDMtIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df.head()"
      ],
      "metadata": {
        "id": "iDTZUBqgNltj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering the movies, we'll perform a slight change of notation, refering to movies as items, to be more generic in terms of recommendation."
      ],
      "metadata": {
        "id": "sHO03OQINE0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df = ratings_df.rename(columns={'movieId': 'itemId'})"
      ],
      "metadata": {
        "id": "g_J5ebLnNffp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_user_id = ratings_df.userId.max()\n",
        "max_item_id = ratings_df.itemId.max()"
      ],
      "metadata": {
        "id": "FEU56F6CM3hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df[\"rating\"] = ratings_df[\"rating\"] / 5.0 "
      ],
      "metadata": {
        "id": "i3wCg06QQD5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce the following splits: 60% train, 20% val, 20% test"
      ],
      "metadata": {
        "id": "M5_SBiC2NwzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val_test = train_test_split(ratings_df, stratify=ratings_df.rating, train_size=0.6)\n",
        "df_val, df_test = train_test_split(df_val_test, stratify=df_val_test.rating, train_size=0.5)"
      ],
      "metadata": {
        "id": "QzWgbRTsNVfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(df):\n",
        "    return torch.utils.data.TensorDataset(\n",
        "        torch.tensor(df[[\"userId\", \"itemId\"]].values), \n",
        "        torch.tensor(df.rating.values).to(torch.float32)\n",
        "    )\n",
        "\n",
        "# Get datasets\n",
        "train_dataset = df_to_dataset(df_train)\n",
        "val_dataset = df_to_dataset(df_val)\n",
        "test_dataset = df_to_dataset(df_test)\n",
        "\n",
        "# Get dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "FXmVK2tbNrsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "77gAl3x_P-Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, cardinalities):\n",
        "        super().__init__()\n",
        "        self.user_embedding_table = torch.nn.Embedding(cardinalities[\"user\"] + 1, embedding_dim)\n",
        "        self.item_embedding_table = torch.nn.Embedding(cardinalities[\"item\"] + 1, embedding_dim)\n",
        "\n",
        "    def interaction(self, user_embeddings, item_embeddings):\n",
        "        # batch-wise dot product \n",
        "        return torch.einsum('bi,bj->b', user_embeddings, item_embeddings)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Assume that data has the form of (uid, iid)\n",
        "        user_ids = data[:, 0]\n",
        "        item_ids = data[:, 1]\n",
        "        # Embed\n",
        "        user_embeddings = self.user_embedding_table(user_ids)\n",
        "        item_embeddings = self.item_embedding_table(item_ids)\n",
        "        # Reconstruct\n",
        "        reconstruction = self.interaction(user_embeddings, item_embeddings)\n",
        "        return reconstruction"
      ],
      "metadata": {
        "id": "8FtnErt_QBBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, device, epoch, print_every=100):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for current_step, (inputs, feedback_gt) in enumerate(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        feedback_gt = feedback_gt.to(device)\n",
        "        # Forward \n",
        "        feedback_pred = model(inputs)\n",
        "        loss = criterion(feedback_pred, feedback_gt)\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "        # Log\n",
        "        losses.append(loss)\n",
        "        if current_step % print_every == 0:\n",
        "            print(f\"[{epoch}, {current_step}] Loss: {loss.item() * 5.0}\")\n",
        "    loss_avg = torch.mean(torch.stack(losses)).item()\n",
        "    return {\"loss\": loss_avg}"
      ],
      "metadata": {
        "id": "ch4D22O0Sd5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for (inputs, feedback_gt) in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        feedback_gt = feedback_gt.to(device)\n",
        "        # Forward \n",
        "        feedback_pred = model(inputs)\n",
        "        loss = criterion(feedback_pred, feedback_gt)\n",
        "        losses.append(loss * 5.0)\n",
        "    loss_avg = torch.mean(torch.stack(losses)).item()\n",
        "    return {\"loss\": loss_avg}"
      ],
      "metadata": {
        "id": "BUqcR3sbVIYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(embedding_dim, {\"user\": max_user_id, \"item\": max_item_id}).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "LFOiqTW4Q2Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "WnsInUitZdMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch_losses = []\n",
        "val_epoch_losses = []\n",
        "for epoch in range(epochs):\n",
        "    train_stats = train_epoch(model, train_loader, optimizer, criterion, device, epoch, print_every=50)\n",
        "    val_stats = evaluate(model, val_loader, device)\n",
        "\n",
        "    train_epoch_losses.append(train_stats[\"loss\"])\n",
        "    val_epoch_losses.append(val_stats[\"loss\"])\n",
        "\n",
        "    print(f\"[{epoch}] Validation loss: {val_stats['loss']}\")"
      ],
      "metadata": {
        "id": "BQNPD5HIUMUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "XAaZtgARJ6ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see the results from training, from training and validation sets"
      ],
      "metadata": {
        "id": "_XinejEbOP5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_epoch_losses, label=\"Train\")\n",
        "plt.plot(val_epoch_losses, label=\"Test\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sg637F5nY34G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And evaluate on the test set"
      ],
      "metadata": {
        "id": "_nUdLbxWONPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_stats = evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "EN13dFa8OOzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test set loss: {test_stats['loss']}\")"
      ],
      "metadata": {
        "id": "qbhpCnBrOYo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a better loss, you could try to find better hyperparameters such as learning rate, optimizer, batch size. You could also try using a bigger dataset such as MovieLens25M."
      ],
      "metadata": {
        "id": "QOSVX2s2HZsK"
      }
    }
  ]
}